{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIDevs2 tasks from newest at the top to oldest at the bottom ,updated daily, to be cleaned ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "from utilities.common import OpenAIClient, AIDevsClient\n",
    "from utilities.config import AI_DEVS_SERVER, AI_DEVS_API_KEY, OPEN_AI_API_KEY\n",
    "\n",
    "BASE_URL = AI_DEVS_SERVER\n",
    "API_KEY = AI_DEVS_API_KEY\n",
    "openai.api_key  = OPEN_AI_API_KEY\n",
    "\n",
    "AIDevs_Client = AIDevsClient(base_url=BASE_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#!pip install --upgrade openai\n",
    "!pip install python-dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inprompt\n",
    "\n",
    "Skorzystaj z API zadania.aidevs.pl, aby pobrać dane zadania inprompt. Znajdziesz w niej dwie właściwości — input, czyli tablicę / listę zdań na temat różnych osób (każde z nich zawiera imię jakiejś osoby) oraz question będące pytaniem na temat jednej z tych osób. Lista jest zbyt duża, aby móc ją wykorzystać w jednym zapytaniu, więc dowolną techniką odfiltruj te zdania, które zawierają wzmiankę na temat osoby wspomnianej w pytaniu. Ostatnim krokiem jest wykorzystanie odfiltrowanych danych jako kontekst na podstawie którego model ma udzielić odpowiedzi na pytanie. Zatem: pobierz listę zdań oraz pytanie, skorzystaj z LLM, aby odnaleźć w pytaniu imię, programistycznie lub z pomocą no-code odfiltruj zdania zawierające to imię. Ostatecznie spraw by model odpowiedział na pytanie, a jego odpowiedź prześlij do naszego API w obiekcie JSON zawierającym jedną właściwość “answer”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"inprompt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'co lubi jeść na śniadanie Alojzy?'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = AIDevs_Client.get_token(task_name)['token']\n",
    "\n",
    "task = AIDevs_Client.get_task(token)\n",
    "people = task['input']\n",
    "question = task['question']\n",
    "\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alojzy'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract name from the question\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Extract name from the question, nothing more:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "name = OpenAIClient.get_completion(prompt, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alojzy ma czarne oczy, krótkie włosy i pracuje jako prawnik, a na śniadanie najbardziej lubi jeść owsiankę']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find list item with contex about name\n",
    "\n",
    "contex = [sentence for sentence in people if name in sentence]\n",
    "\n",
    "contex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alojzy najbardziej lubi jeść owsiankę na śniadanie.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Answer the question\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{contex}, {question}\n",
    "\"\"\"\n",
    "\n",
    "answer = OpenAIClient.get_completion(prompt)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 0, 'msg': 'OK', 'note': 'CORRECT'}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = AIDevs_Client.submit_answer(token, answer)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# liar\n",
    "\n",
    "Jest to mechanizm, który mówi nie na temat w 1/3 przypadków. Twoje zadanie polega na tym, aby do endpointa /task/ wysłać swoje pytanie w języku angielskim (dowolne, np “What is capital of Poland?’) w polu o nazwie ‘question’ (metoda POST, jako zwykłe pole formularza, NIE JSON). System API odpowie na to pytanie (w polu ‘answer’) lub zacznie opowiadać o czymś zupełnie innym, zmieniając temat. Twoim zadaniem jest napisanie systemu filtrującego (Guardrails), który określi (YES/NO), czy odpowiedź jest na temat. Następnie swój werdykt zwróć do systemu sprawdzającego jako pojedyncze słowo YES/NO. Jeśli pobierzesz treść zadania przez API bez wysyłania żadnych dodatkowych parametrów, otrzymasz komplet podpowiedzi. Skąd wiedzieć, czy odpowiedź jest ‘na temat’? Jeśli Twoje pytanie dotyczyło stolicy Polski, a w odpowiedzi otrzymasz spis zabytków w Rzymie, to odpowiedź, którą należy wysłać do API to NO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"liar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the moon?'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the moon?\"\n",
    "#question = OpenAIClient.get_completion(\"respond with random question\", model=\"gpt-3.5-turbo\", max_tokens=100, temperature=0.8)\n",
    "\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIDevs_Client = AIDevsClient(base_url=BASE_URL, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(token, question):\n",
    "    url = f\"{BASE_URL}/task/{token}\"\n",
    "    payload = {\n",
    "        \"question\": question\n",
    "    }\n",
    "    response = requests.post(url, data=payload)  # Using data instead of json\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Return the entire JSON response\n",
    "    else:\n",
    "        print(f\"Error asking question: {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the moon? : The moon is a natural satellite that orbits around the Earth. It is the Earth's only natural satellite and is located about 384,400 kilometers away from our planet. The moon is approximately 4.5 billion years old and has a diameter of about 3,474 kilometers. It plays a significant role in various aspects, including tides, lunar cycles, and even cultural and religious beliefs. The moon also has a barren and rocky surface, with no atmosphere or liquid water.\n"
     ]
    }
   ],
   "source": [
    "token = AIDevs_Client.get_token(task_name)['token']\n",
    "\n",
    "answer_to_question = ask_question(token, question)['answer']\n",
    "\n",
    "print(question + \" : \" + answer_to_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the question '{question}' and the answer '{answer_to_question}', is the answer relevant? Respond with YES or NO.\n",
    "\"\"\"\n",
    "\n",
    "response = OpenAIClient.get_completion(prompt, model=\"gpt-3.5-turbo\", max_tokens=100, temperature=0.2)\n",
    "\n",
    "response.strip()  # Stripping to ensure there's no extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 0, 'msg': 'OK', 'note': 'CORRECT'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AIDevs_Client.submit_answer(token, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blogger\n",
    "\n",
    "Napisz wpis na bloga (w języku polskim) na temat przyrządzania pizzy Margherity. Zadanie w API nazywa się ”blogger”. Jako wejście otrzymasz spis 4 rozdziałów, które muszą pojawić się we wpisie. Jako odpowiedź musisz zwrócić tablicę (w formacie JSON) złożoną z 4 pól reprezentujących te cztery rozdziały."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"blogger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wstęp: kilka słów na temat historii pizzy', 'Niezbędne składniki na pizzę', 'Robienie pizzy', 'Pieczenie pizzy w piekarniku']\n"
     ]
    }
   ],
   "source": [
    "token = AIDevs_Client.get_token(task_name)['token']\n",
    "task = AIDevs_Client.get_task(token)['blog']\n",
    "\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Początki pizzy sięgają starożytnych czasów, kiedy to mieszkańcy Bliskiego Wschodu piekli na płaskich kamieniach chleb, na którym umieszczali różne dodatki. Jednak pizza, którą znamy i kochamy dzisiaj, pochodzi z Neapolu w Italii. W połowie XIX wieku, kiedy to Włochy były zjednoczone, królowa Margherita odwiedziła Neapol. Na jej cześć miejscowy piekarz, Raffaele Esposito, stworzył specjalną pizzę, na której umieścił składniki w barwach włoskiej flagi: czerwone pomidory, białą mozzarellę i zielone liście bazylii. Ta pizza, nazwana na cześć królowej Margherity, stała się prototypem dla wszystkich pizz margherita, które znamy i uwielbiamy dzisiaj.', 'Tworzenie idealnej pizzy margherita zaczyna się od zebrania odpowiednich składników. Na początek potrzebujesz wysokiej jakości mąki typu 00, drożdży, soli i wody do przygotowania podstawy, czyli ciasta na pizzę. Następnie, kluczowym składnikiem jest sos pomidorowy, najlepiej przygotowany z dojrzałych, świeżych pomidorów San Marzano. Kolejnym niezbędnym składnikiem jest świeża mozzarella - najlepiej użyć mozzarelli di bufala. Na koniec, nie zapomnij o liściach świeżej bazylii i odrobinie oliwy z oliwek extra vergine. Te składniki zapewnią ci autentyczne, włoskie smaki pizzy margherita.', 'Robienie pizzy margherita to prawdziwa sztuka, która wymaga precyzyjności, cierpliwości i miłości do kulinariów. Począwszy od przygotowania idealnie elastycznego ciasta, poprzez wybór najwyższej jakości składników, takich jak świeże pomidory, bazylię i mozzarellę, aż po doskonałe wypieczenie w piecu. Każdy etap ma kluczowe znaczenie dla ostatecznego smaku i konsystencji pizzy. Włoska pizza margherita jest symbolem prostoty i doskonałości, a jej przygotowanie to prawdziwa przyjemność dla każdego miłośnika pizzy.', 'Pieczenie pizzy margherita w piekarniku to proces, który wymaga precyzji i umiejętności. Idealna pizza powinna być chrupiąca na zewnątrz, a miękka i soczysta w środku. Aby to osiągnąć, piekarnik musi być dobrze rozgrzany - zaleca się temperaturę około 250 stopni Celsjusza. Ważne jest również, aby pizza była równomiernie rozłożona na blasze, aby składniki mogły równomiernie się upiec. Czas pieczenia to zazwyczaj około 10-15 minut, ale zawsze warto obserwować pizzę, aby upewnić się, że nie spali się na brzegach. Pamiętaj, że każdy piekarnik działa nieco inaczej, więc może być konieczne dostosowanie temperatury i czasu pieczenia do twojego konkretnego urządzenia.']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "        You are a pizza expert that writes a blog about pizza margherita in polish.\n",
    "        Write a short paragraph about the given topic:\n",
    "        \"\"\"\n",
    "        \n",
    "response = OpenAIClient.get_completion(prompt, model=\"gpt-3.5-turbo\", max_tokens=1000, temperature=0.3)\n",
    "\n",
    "\n",
    "blog_post = [OpenAIClient.get_completion(prompt+topic) for topic in task]\n",
    "\n",
    "print(blog_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 0, 'msg': 'OK', 'note': 'CORRECT'}\n"
     ]
    }
   ],
   "source": [
    "result = AIDevs_Client.submit_answer(token, blog_post)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# moderation\n",
    "\n",
    "Zastosuj wiedzę na temat działania modułu do moderacji treści i rozwiąż zadanie o nazwie “moderation” z użyciem naszego API do sprawdzania rozwiązań. Zadanie polega na odebraniu tablicy zdań (4 sztuki), a następnie zwróceniu tablicy z informacją, które zdania nie przeszły moderacji. Jeśli moderacji nie przeszło pierwsze i ostatnie zdanie, to odpowiedź powinna brzmieć [1,0,0,1]. Pamiętaj, aby w polu ‘answer’ zwrócić tablicę w JSON, a nie czystego stringa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"moderation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ten gość musi zginąć. Nie pozwole sobię na obrażanie mnie.', 'Sasha.Grey.s3x.p0rn.extreme-interracial.S03E12.DVDRip.mp4', 'majonez Winiary jest lepszy od Kieleckiego', 'azjaci są głupi i brzydcy i nie powinni żyć']\n"
     ]
    }
   ],
   "source": [
    "token = AIDevs_Client.get_token(task_name)['token']\n",
    "task = AIDevs_Client.get_task(token)['input']\n",
    "\n",
    "print(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the moderation results\n",
    "moderation_results = []\n",
    "\n",
    "# Iterate through each item in the zadanie list\n",
    "for item in task:\n",
    "    # Send the item to the Moderation endpoint\n",
    "    response = openai.Moderation.create(input=item)\n",
    "    \n",
    "    # Access the 'flagged' value from the response\n",
    "    flagged = response['results'][0]['flagged']\n",
    "    \n",
    "    # Append 1 if flagged, else 0, to the moderation_results list\n",
    "    moderation_results.append(1 if flagged else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moderation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 0, 'msg': 'OK', 'note': 'CORRECT'}\n"
     ]
    }
   ],
   "source": [
    "result = AIDevs_Client.submit_answer(token, moderation_results)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helloapi\n",
    "\n",
    "Komunikacja z API odbywa się z pomocą kodu oraz formatu JSON, a każde z zadań składa się z trzech części:\n",
    "\n",
    "autoryzacji\n",
    "\n",
    "pobierania danych wejściowych (string lub tablica obiektów)\n",
    "\n",
    "odesłania odpowiedzi (właściwość answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"helloapi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 0, 'msg': 'OK', 'note': 'CORRECT'}\n"
     ]
    }
   ],
   "source": [
    "token = AIDevs_Client.get_token(task_name)['token']\n",
    "cookie = AIDevs_Client.get_task(token)['cookie']\n",
    "result = AIDevs_Client.submit_answer(token, cookie)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
